import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots


import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF
from statsmodels.stats.anova import anova_lm


from ISLP import load_data
from ISLP.models import (ModelSpec as MS, summarize, poly) 


Auto = load_data('Auto')
Auto.head()


y = Auto['mpg']
X = MS(['horsepower']).fit_transform(Auto)
model = sm.OLS(y, X)
results = model.fit()
summarize(results)


newX = pd.DataFrame({'horsepower': [98]})
newX = MS(['horsepower']).fit_transform(newX)
newX


new_prediction = results.get_prediction(newX)
new_prediction.predicted_mean


new_prediction.conf_int(alpha=0.05)


new_prediction.conf_int(obs=True, alpha=0.05)





def abline(ax, b, m):
    "Add a line with slope m and intercept b to ax"
    xlim = ax.get_xlim()
    ylim = [m * xlim[0] + b, m * xlim[1] + b]
    ax.plot(xlim, ylim)


def abline(ax, b, m, *args, **kwargs):
    "Add a line with slope m and intercept b to ax"
    xlim = ax.get_xlim()
    ylim = [m * xlim[0] + b, m * xlim[1] + b]
    ax.plot(xlim, ylim, *args, **kwargs)


ax = Auto.plot.scatter('horsepower', 'mpg')
abline(ax, results.params.iloc[0], results.params.iloc[1], 'r--', linewidth=3)


ax = subplots(figsize=(8,8))[1]
ax.scatter(results.fittedvalues, results.resid)
ax.set_xlabel('Fitted Value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--');


infl = results.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
ax.axhline(0, c='k', ls='--')
np.argmax(infl.hat_matrix_diag)


infl = results.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(results.resid, infl.hat_matrix_diag)
ax.set_xlabel('Residual')
ax.set_ylabel('Leverage')
ax.axhline(0, c='k', ls='--')
np.argmax(infl.hat_matrix_diag)





Auto.columns


pd.plotting.scatter_matrix(Auto, figsize=(15,15));


Auto.corr()


terms = Auto.columns.drop('mpg')
X = MS(terms).fit_transform(Auto)
model1 = sm.OLS(y, X)
results1 = model1.fit()
summarize(results1)


anova_lm(results, results1)





ax = subplots(figsize=(8,8))[1]
ax.scatter(results1.fittedvalues, results1.resid)
ax.set_xlabel('Fitted Value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--');





infl = results1.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
ax.axhline(0, c='k', ls='--')
np.argmax(infl.hat_matrix_diag)





X = MS(['horsepower', 'year', ('horsepower', 'year')]).fit_transform(Auto)
model2 = sm.OLS(y, X)
summarize(model2.fit())


X = MS(['horsepower', 'year', 'acceleration', 'weight', ('horsepower', 'year'), ('acceleration', 'weight')]).fit_transform(Auto)
model2 = sm.OLS(y, X)
summarize(model2.fit())


allvars = list(Auto.columns.drop('mpg'))
final = allvars + [('cylinders','displacement'),('year','origin')]
X = MS(final).fit_transform(Auto)
model2 = sm.OLS(y, X)
summarize(model2.fit())





X = MS([poly('horsepower', degree=2), 'year']).fit_transform(Auto)
model3 = sm.OLS(y, X)
summarize(model3.fit())


X = MS([poly('horsepower', degree=1), 'year']).fit_transform(Auto)
model3 = sm.OLS(y, X)
summarize(model3.fit())


X = MS([poly('horsepower', degree=3), 'year']).fit_transform(Auto)
model3 = sm.OLS(y, X)
summarize(model3.fit())





Carseats = load_data('Carseats')
Carseats.columns


X = MS(['Price', 'Urban', 'US']).fit_transform(Carseats)
y = Carseats['Sales']
model4 = sm.OLS(y, X)
results4 = model4.fit()
summarize(results4)











X = MS(['Price', 'US']).fit_transform(Carseats)
model5 = sm.OLS(y, X)
results5 = model5.fit()
summarize(results5)


anova_lm(results4, results5)





results5.conf_int(alpha=0.05)


ax = subplots(figsize=(8,8))[1]
ax.scatter(results5.fittedvalues, results5.resid)
ax.set_xlabel('Fitted Value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--');





infl = results5.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
ax.axhline(0, c='k', ls='--')
np.argmax(infl.hat_matrix_diag)





rng = np.random.default_rng(1)
x = rng.normal(size=100)
y = 2 * x + rng.normal(size=100)


df = pd.DataFrame({'x': x, 'y': y})
X = MS(['x'],intercept=False).fit_transform(df)
y = df['y']
model6 = sm.OLS(y, X)
results6 = model6.fit()
summarize(results6)





model6 = sm.OLS(X, y)
results6 = model6.fit()
summarize(results6)











# Generate n = 100 observations
n = 100

# Generate X and Y with different variances
X = rng.normal(0, 1, n)
Y = 2 * X + rng.normal(0, 5, n)  # Y has a larger variance due to the noise

# Regression of Y onto X
model_Y_on_X = sm.OLS(Y, X).fit()
beta_Y_on_X = model_Y_on_X.params[0]

# Regression of X onto Y
model_X_on_Y = sm.OLS(X, Y).fit()
beta_X_on_Y = model_X_on_Y.params[0]

beta_Y_on_X, beta_X_on_Y





# Generate n = 100 observations
n = 100

# Generate X and Y with the same variance
X = rng.normal(0, 1, n)
Y = 2 * X + rng.normal(0, 1, n)  # Same variance as X

# Regression of Y onto X
model_Y_on_X = sm.OLS(Y, X).fit()
beta_Y_on_X = model_Y_on_X.params[0]

# Regression of X onto Y
model_X_on_Y = sm.OLS(X, Y).fit()
beta_X_on_Y = model_X_on_Y.params[0]

beta_Y_on_X, beta_X_on_Y





rng = np.random.default_rng(1)

n = 100

X = rng.normal(0, 1, n)
eps = rng.normal(0, np.sqrt(0.25), n)

y = -1 + 0.5 * X + eps
y[:10]





ax = subplots(figsize=(8,8))[1]
ax.scatter(X, y)
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('X vs. Y');





df = pd.DataFrame({'x': X, 'y': y})
X = MS(['x']).fit_transform(df)
y = df['y']
model_0 = sm.OLS(y, X)
results_0 = model_0.fit()
summarize(results_0)





slope = results_0.params['x']
intercept = results_0.params['intercept']

# Generate x-values for the regression line
x_line = np.linspace(min(X['x']), max(X['x']), 100)

# Calculate y-values for the regression line
y_line = intercept + slope * x_line

ax = subplots(figsize=(8,8))[1]
ax.scatter(X['x'], y)

# Plot the least squares line
ax.plot(x_line, y_line, label='Least Squares Line')

true_slope = slope
true_intercept = intercept
y_pop = true_intercept + true_slope * x_line
ax.plot(x_line, y_pop, label='Population Regression Line')

ax.legend();


X = MS([poly('x', degree=2)]).fit_transform(df)
y = df['y']
model_1 = sm.OLS(y, X)
results_1 = model_1.fit()
summarize(results_1)





n = 100

X = rng.normal(0, 1, n)
eps = rng.normal(0, np.sqrt(0.2), n)

y = -1 + 0.5 * X + eps
y[:10]





ax = subplots(figsize=(8,8))[1]
ax.scatter(X, y)
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('X vs. Y');





df = pd.DataFrame({'x': X, 'y': y})
X = MS(['x']).fit_transform(df)
y = df['y']
model_0 = sm.OLS(y, X)
results_0 = model_0.fit()
summarize(results_0)





slope = results_0.params['x']
intercept = results_0.params['intercept']

# Generate x-values for the regression line
x_line = np.linspace(min(X['x']), max(X['x']), 100)

# Calculate y-values for the regression line
y_line = intercept + slope * x_line

ax = subplots(figsize=(8,8))[1]
ax.scatter(X['x'], y)

# Plot the least squares line
ax.plot(x_line, y_line, label='Least Squares Line')

true_slope = slope
true_intercept = intercept
y_pop = true_intercept + true_slope * x_line
ax.plot(x_line, y_pop, label='Population Regression Line')

ax.legend();


X = MS([poly('x', degree=2)]).fit_transform(df)
y = df['y']
model_1 = sm.OLS(y, X)
results_1 = model_1.fit()
summarize(results_1)





n = 100

X = rng.normal(0, 1, n)
eps = rng.normal(0, np.sqrt(0.75), n)

y = -1 + 0.5 * X + eps
y[:10]





ax = subplots(figsize=(8,8))[1]
ax.scatter(X, y)
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('X vs. Y');





df = pd.DataFrame({'x': X, 'y': y})
X = MS(['x']).fit_transform(df)
y = df['y']
model_0 = sm.OLS(y, X)
results_0 = model_0.fit()
summarize(results_0)





slope = results_0.params['x']
intercept = results_0.params['intercept']

# Generate x-values for the regression line
x_line = np.linspace(min(X['x']), max(X['x']), 100)

# Calculate y-values for the regression line
y_line = intercept + slope * x_line

ax = subplots(figsize=(8,8))[1]
ax.scatter(X['x'], y)

# Plot the least squares line
ax.plot(x_line, y_line, label='Least Squares Line')

true_slope = slope
true_intercept = intercept
y_pop = true_intercept + true_slope * x_line
ax.plot(x_line, y_pop, label='Population Regression Line')

ax.legend();


X = MS([poly('x', degree=2)]).fit_transform(df)
y = df['y']
model_1 = sm.OLS(y, X)
results_1 = model_1.fit()
summarize(results_1)





rng = np.random.default_rng(10)
x1 = rng.uniform(0, 1, size=100)
x2 = 0.5 * x1 + rng.normal(size=100) / 10
y = 2 + 2 * x1 + 0.3 * x2 + rng.normal(size=100)





# Calculating the correlation between x1 and x2
correlation = np.corrcoef(x1, x2)[0,1]

# Creating a scatterplot to display the relationship between x1 and x2
ax = subplots(figsize=(8, 6))[1]
ax.scatter(x1, x2, color='blue')
ax.set_title('Scatterplot of x1 vs x2')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.grid(True)
correlation





df = pd.DataFrame({'x1': x, 'x2': x2, 'y': y})
y = df['y']
X = MS(['x1', 'x2']).fit_transform(df)
model = sm.OLS(y, X)
results = model.fit()
summarize(results)


beta_hat_0, beta_hat_1, beta_hat_2 = results.params
summary = results.summary()

beta_hat_0, beta_hat_1, beta_hat_2, summary





X = MS(['x1']).fit_transform(df)
model1 = sm.OLS(y, X)
results2 = model1.fit()
summarize(results2)






